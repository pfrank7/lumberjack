---
layout: page
title : Docs
header : Docs
order : 10
group : navigation
description: "Lumberjack Docs"
---
{% include JB/setup %}
<div class = "col-sm-1">
  <div class = "menu_open" id = "myMenu" >
    <span style="font-size:30px;cursor:pointer" onclick="openNav()">&#9776; menu</span>
  </div>
</div>
<div id = "mySidenav" class="sidenav">
  <a href="javascript:void(0)" class="closebtn" id = "close" onclick="closeNav()">&times;</a>
  <a href = "#About">About</a>
  <a href="#BuildTree">BuildTree</a>
  <a href="#ComputeSimilarity">ComputeSimilarity</a>
  <a href="#FeatureImportance">FeatureImportance</a>
  <a href="#mnist">mnist</a>
  <a href="#OOBPredict">OOBPredict</a>
  <a href="#PackForest">PackForest</a>
  <a href="#PackPredict">PackPredict</a>
  <a href="#Predict">Predict</a>
  <a href="#RandMat">RandMat</a>
  <a href="#RandMatCat">RandMatCat</a>
  <a href="#RerF">RerF</a>
  <a href="#RunFeatureImportance">RunFeatureImportance</a>
  <a href="#RunOOB">RunOOB</a>
  <a href="#RunPredict">RunPredict</a>
  <a href="#RunPredictLeaf">RunPredictLeaf</a>
  <a href="#StrCorr">StrCorr</a>
</div>
<div class = "col-sm-11">
<div class="main_content">
  <h2>Package 'rerf'</h2>
  <p>Navigate through the links on the left. The full pdf documentation can be found on <a href = "https://cran.r-project.org/web/packages/rerf/rerf.pdf"> cran. </a></p>
  <span class="anchor" id="About"></span>
  <div class = "aboutSect"> 
    <h3> About </h3> 
    <p><b> Type </b>Package</p>
    <p><b> Title </b>Randomer Forest </p>
    <p><b> Version </b>1.1.3</p>
    <p><b> Date </b>2018-07-05</p>
    <p><b> Description </b> Random Forester (RerF) is an algorithm developed by Tomita
      (2016) <arXiv:1506.03410v2> which is similar to Random Forest - Random
      Combination (Forest-RC) developed by Breiman (2001)
      <doi:10.1023/A:1010933404324>. Random Forests create axis-parallel, or
      orthogonal trees. That is, the feature space is recursively split along
      directions parallel to the axes of the feature space. Thus, in cases in
      which the classes seem inseparable along any single dimension, Random
      Forests may be suboptimal. To address this, Breiman also proposed and
      characterized Forest-RC, which uses linear combinations of coordinates
      rather than individual coordinates, to split along. This package,
      'rerf', implements RerF which is similar to Forest-RC. The difference
      between the two algorithms is where the random linear combinations
      occur: Forest-RC combines features at the per tree level whereas RerF
      takes linear combinations of coordinates at every node in the tree.</p>
      <p><b> License </b>GPL-2</p>
    <p><b> URL </b><a hrerf = "https://github.com/neurodata/R-RerF">https://github.com/neurodata/R-RerF</a></p>
    <p><b> Imports </b>parallel, RcppZiggurat, utils, stats, dummies</p>
    <p><b> LinkingTo </b>Rcpp, RcppArmadillo</p>
    <p><b> SystemRequirements </b>GNU make</p>
    <p><b> ByteCompile </b>true</p>
    <p><b> RoxygenNote </b>6.0.1</p>
    <p><b> NeedsCompilation </b>yes</p>
    <p><b> Author </b>James Browne [aut, cre],
     Tyler Tomita [aut],
     Joshua Vogelstein [ths]</p>
    <p><b> Maintainer </b>James Browne <i>jbrowne6@jhu.edu</i> </p>
    <p><b> Depends </b>R (>= 2.10)</p>
    <p><b> Repository </b>CRAN</p>
    <p><b> Date/Publication </b>2018-07-29 06:10:03 UTC</p>
  
  
  </div>
  <span class="anchor" id = "BuildTree"></span>
  <div class = "docSect">
    <h4> BuildTree - RerF Tree Generator </h4>
    <h5> Description </h5>
    <p> Creates a single decision tree based on an input matrix and class vector. This is the function used
      by rerf to generate trees.</p>
    <h5> Usage </h5>
    <p><code> BuildTree(X, Y, min.parent, max.depth, bagging, replacement, stratify,
      class.ind, class.ct, fun, mat.options, store.oob, store.impurity, progress,
        rotate) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> Y </code></p>
      <p><code> min.parent </code></p>
      <p><code> max.depth </code></p>
      <p><code> bagging </code></p>
      <p><code> replacement </code></p>
      <p><code> stratify </code></p>
      <p><code> class.ind </code></p>
      <p><code> class.ct  </code></p>
      <p><code> fun </code></p>
      <p><code> mat.options  </code></p>
      <p><code> store.oob </code></p>
      <p><code> store.impurity  </code></p>
      <p><code> progress  </code></p>
      <p><code> rotate </code></p>
    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame. The rows correspond to
        observations and columns correspond to features. </p>
      <p> an n length vector of class labels. Class labels must be integer or numeric and
        be within the range 1 to the number of classes. </p>
      <p> the minimum splittable node size. A node size < min.parent will be a leaf node.
        (min.parent = 6) </p>
      <p> the longest allowable distance from the root of a tree to a leaf node (i.e. the
        maximum allowed height for a tree). If max.depth=0, the tree will be allowed to
        grow without bound. (max.depth=0) </p>
      <p> a non-zero value means a random sample of X will be used during tree creation.
        If replacement = FALSE the bagging value determines the percentage of samples to leave out-of-bag. 
        If replacement = TRUE the non-zero bagging value is ignored.(bagging=.2) </p>
      <p> if TRUE then n samples are chosen, with replacement, from X. (replacement=TRUE) </p>
      <p> if TRUE then class sample proportions are maintained during the random sampling.
        Ignored if replacement = FALSE. (stratify = FALSE). </p>
      <p> a vector of lists. Each list holds the indexes of its respective class (e.g. list 1
        contains the index of each class 1 sample). </p>
      <p> a cumulative sum of class counts. </p>
      <p> a function that creates the random projection matrix. (fun=NULL) </p>
      <p> a list of parameters to be used by fun. (mat.options=c(ncol(X), round(ncol(X)^.5),1L,
        1/ncol(X))) </p>
      <p> if TRUE then the samples omitted during the creation of a tree are stored as part
        of the tree. This is required to run OOBPredict(). (store.oob=FALSE) </p>
      <p> if TRUE then the reduction in Gini impurity is stored for every split. This is
        required to run FeatureImportance() (store.impurity=FALSE) </p>
      <p> if true a pipe is printed after each tree is created. This is useful for large datasets.
        (progress=FALSE) </p>
      <p> if TRUE then the data matrix X is uniformly randomly rotated. (rotate=FALSE) </p>
      
    </div>
    <h5> Value </h5>
    <p> Tree </p>
  </div>
    
    
  <span class="anchor" id = "ComputeSimilarity"></span>
  <div class = "docSect">
    <h4> ComputeSimilarity - Compute Similarities </h4>
    <h5> Description </h5>
    <p> Computes pairwise similarities between observations. The similarity between two points is defined
      as the fraction of trees such that two points fall into the same leaf node.</p>
    <h5> Usage </h5>
    <p><code> ComputeSimilarity(X, forest, num.cores = 0L, Xtrain = NULL) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
      <p><code> Xtrain </code></p>
    </div>
    <div class = "argRight">
      <p> an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest. </p>
      <p> forest a forest trained using the rerf function, with COOB=TRUE. </p>
      <p> num.cores the number of cores to use while training. If num.cores=0 then 1 less 
        than the number of cores reported by the OS are used. (num.cores=0) </p>
      <p> Xtrain an n by d numeric matrix (preferable) or data frame. This should be the same data matrix/frame used 
        to train the forest, and is only required if RerF was called with rank.transform = TRUE. (Xtrain=NULL) </p>
    </div>
    <h5> Value </h5>
    <p> similarity a normalized n by n matrix of pairwise similarities </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> X &lt- as.matrix(iris[,1:4]) </code></p>
    <p><code> Y &lt- iris[[5L]] </code></p>
    <p><code> forest &lt- RerF(X, Y, num.cores = 1L) </code></p>
    <p><code> sim.matrix &lt- ComputeSimilarity(X, forest, num.cores = 1L) </code></p> 
  </div>
    
    
  <span class="anchor" id = "FeatureImportance"></span>
  <div class = "docSect">
    <h4> FeatureImportance - Compute Feature Importance of a RerF model </h4>
    <h5> Description </h5>
    <p> Computes feature importance of every unique feature used to make a split in the RerF model.</p>
    <h5> Usage </h5>
    <p><code> FeatureImportance(forest, num.cores = 0L) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
    </div>
    <div class = "argRight">
      <p> a forest trained using the RerF function with argument store.impurity = TRUE </p>
      <p> number of cores to use. If num.cores = 0, then 1 less than the number of cores
        reported by the OS are used. (num.cores = 0) </p>
    </div>
    <h5> Value </h5>
    <p> feature.imp </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> forest &lt- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L, store.impurity = TRUE) </code></p>
    <p><code> feature.imp &lt- FeatureImportance(forest, num.cores = 1L) </code></p>
  </div>
    
    
  <span class="anchor" id = "mnist"></span> 
  <div class = "docSect">
    <h4> mnist - A subset of the MNIST dataset for handwritten digit classification </h4>
    <h5> Description </h5>
    <p> A dataset consiting of 10 percent of the MNIST training set and the full test set.</p>
    <h5> Usage </h5>
    <p><code> data(mnist) </code></p>
    <h5> Format </h5>
    <p> A list with four items: Xtrain is a training set matrix with 6000 rows (samples) and 784 columns
      (features), Xtrain is an integer array of corresponding training class labels, Xtest is a test set matrix
      of 10000 rows and 784 columns, and Ytest is the corresponding class labels. Rows in Xtrain and
      Xtest correspond to different images of digits, and columns correspond to the pixel intensities in
      each image, obtained by flattening the image pixels in column-major ordering. </p>
    <h5> Source </h5>
    <p><a href = "http://yann.lecun.com/exdb/mnist/"> MNIST </a></p>
    <h5> References </h5>
    <p> Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. "Gradient-based learning applied to document
      recognition." Proceedings of the IEEE, 86(11):2278-2324, November 1998. </p>
    <h5> Examples </h5>
    <p><code> data(mnist) </code></p>
  </div>
    
    
  <span class="anchor" id = "OOBPredict"></span> 
  <div class = "docSect">
    <h4> OOBPredict - Compute out-of-bag predictions </h4>
    <h5> Description </h5>
    <p> Computes out-of-bag class predictions for a forest trained with store.oob=TRUE.</p>
    <h5> Usage </h5>
    <p><code> OOBPredict(X, forest, num.cores = 0L, output.scores = FALSE) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
      <p><code> output.scores </code></p>
    </div>
    <div class = "argRight">
      <p> an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest. </p>
      <p> a forest trained using the RerF function, with store.oob=TRUE. </p>
      <p> number of cores to use. If num.cores = 0, then 1 less than the number of cores
        reported by the OS are used. (num.cores = 0) </p>
      <p> if TRUE then predicted class scores (probabilities) for each observation are returned
        rather than class labels. (output.scores = FALSE) </p>
    </div>
    <h5> Value </h5>
    <p> predictions a length n vector of predictions in a format similar to the Y vector used to train the forest </p>
    <h5> Examples </h5>
    <p><code> library(rerf) </code></p>
    <p><code> X <- as.matrix(iris[,1:4]) </code></p>
    <p><code> Y <- iris[[5L]] </code></p>
    <p><code> forest <- RerF(X, Y, store.oob=TRUE, num.cores = 1L) </code></p>
    <p><code> predictions <- OOBPredict(X, forest, num.cores = 1L) </code></p>
    <p><code> oob.error <- mean(predictions != Y) </code></p>
  </div>
    
    
  <span class="anchor" id = "PackForest"></span> 
  <div class = "docSect">
    <h4> PackForest - Packs a forest and saves modified forest to disk for use by PackPredict
      function </h4>
    <h5> Description </h5>
    <p> Efficiently packs a forest trained with the RF option. Two intermediate data structures are written
      to disk, forestPackTempFile.csv and traversalPackTempFile.csv. The size of these data structures is
      proportional to a trained forest and training data respectively. Both data structures are removed at
      the end of the operation. The resulting forest is saved as forest.out. The size of this file is similar to
      the size of the trained forest.</p>
    <h5> Usage </h5>
    <p><code> PackForest(X, Y, forest) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> Y </code></p>
      <p><code> forest </code></p>
    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame used to train the forest. </p>
      <p> a numeric vector of size n. If the Y vector used to train the forest was not of type
        numeric then a simple call to as.numeric(Y) will suffice as input. </p>
      <p> a forest trained using the RerF function, with store.oob=TRUE. </p>
    </div>
  </div>
    
    
  <span class="anchor" id = "PackPredict"></span>     
  <div class = "docSect">
    <h4> Predict - Compute class predictions for each observation in X</h4>
    <h5> Description </h5>
    <p> Predicts the classification of samples using a trained forest.</p>
    <h5> Usage </h5>
    <p><code> PackPredict(X, num.cores = 1) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> num.cores </code></p>
    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame. The rows correspond to
        observations and columns correspond to features of a test set, which should be
        different from the training set. </p>
      <p> forest a forest trained using the rerf function, with COOB=TRUE. </p>
      <p> the number of cores to use while predicting. (num.cores=0) </p>
    </div>
    <h5> Value </h5>
    <p> predictions an n length vector of prediction class numbers </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> trainIdx <- c(1:40, 51:90, 101:140) </code></p>
    <p><code> X <- as.matrix(iris[,1:4]) </code></p>
    <p><code> Y <- as.numeric(iris[,5]) </code></p>
    <p><code> forest <- RerF(X,Y, mat.options = list(p = ncol(X), d =ceiling(sqrt(ncol(X))), random.matrix = "rf", rho = 1/ncol(X)), rfPack=TRUE, num.cores=2) </code></p>
    <p><code> predictions <- PackPredict(X) </code></p>
  </div>
    
    
  <span class="anchor" id = "Predict"></span>     
  <div class = "docSect">
    <h4> Predict - Compute class predictions for each observation in X </h4>
    <h5> Description </h5>
    <p> Predicts the classification of samples using a trained forest.</p>
    <h5> Usage </h5>
    <p><code> Predict(X, forest, num.cores = 0L, Xtrain = NULL, aggregate.output = TRUE, output.scores = FALSE) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
      <p><code> Xtrain </code></p>
      <p><code> aggregate.output </code></p>
      <p><code> output.scores </code></p>
    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame. The rows correspond to
        observations and columns correspond to features of a test set, which should be
        different from the training set. </p>
      <p> a forest trained using the RerF function. </p>
      <p> the number of cores to use while training. If NumCores=0 then 1 less than the
        number of cores reported by the OS are used. (NumCores=0) </p>
      <p> an n by d numeric matrix (preferable) or data frame. This should be the same
        data matrix/frame used to train the forest, and is only required if RerF was called
        with rank.transform = TRUE. (Xtrain=NULL) </p>
      <p> if TRUE then the tree predictions are aggregated via majority vote. Otherwise,
        the individual tree predictions are returned. (aggregate.output=TRUE) (num.cores=0) </p>
      <p> if TRUE then predicted class scores (probabilities) for each observation are returned
        rather than class labels. (output.scores = FALSE) </p>
    </div>
    <h5> Value </h5>
    <p> predictions an n length vector of predictions </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> trainIdx <- c(1:40, 51:90, 101:140) </code></p>
    <p><code> X <- as.matrix(iris[,1:4]) </code></p>
    <p><code> Y <- as.numeric(iris[,5]) </code></p>
    <p><code> forest <- RerF(X[trainIdx, ], Y[trainIdx], num.cores = 1L, rank.transform = TRUE) # Using a set of samples with unknown classification </code></p>
    <p><code> predictions <- Predict(X[-trainIdx, ], forest, num.cores = 1L, Xtrain = X[trainIdx, ]) </code></p>
    <p><code> error.rate <- mean(predictions != Y[-trainIdx]) </code></p>
  </div>  
  
    
  <span class="anchor" id = "RandMat"></span> 
  <div class = "docSect">
    <h4> RandMat - Create a Random Matrix </h4>
    <h5> Description </h5>
    <p> Create a random matrix. At each node of a tree, a p-by-d random matrix is sampled and used to
      generate a new set of d features, each of which is a linear combination of the original p features.
      Thus, the columns of the random matrix can be viewed as a set of bases in a new feature space.</p>
    <h5> Usage </h5>
    <p><code> RandMat(mat.options) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p style ="height:500px;"><code> mat.options </code></p>
    </div>
    <div class = "argRight">
      <p style ="height:500px;"> a list of parameters specifying the distribution for sampling the random matrix.
        The first element specifies the dimensionality p of the data (# features). The
        second element specifies the number of columns d in the random matrix. The
        third element specifies the "type" of distribution. The types of distributions supported
        thus far are: "binary", "continuous", "poisson", "rf", "frc", and "frcn."
        The fourth element specifies the average density (proportion of nonzeros) rho of
        the random matrix. For the "binary" distribution, rho is a real number between 0
        and 1. rho*d*p elements of the matrix are randomly chosen to be nonzero. Each
        nonzero is randomly assigned -1 or 1 with equal probability. The "continuous"
        distribution is the same as "binary", except the nonzeros are sampled iid from
        the standard normal distribution. "poisson" samples the number of nonzeros for
        each of the d columns from a poisson(rho) distribution (rho is an integer >= 1).
        The location of nonzeros in each column is randomly chosen, and the nonzeros
        are assigned -1 or 1 with equal probability. "rf" randomly samples d indices
        in 1,...,p without replacement (d <= p is required). Each of the d columns in
        the random matrix has a single nonzero placed at the respective sampled index.
        This is equivalent to the canonical random forest algorithm, which subsamples
        variables at each node. Note that rho is irrelevant. "frc" is Breiman’s forest-RC
        algorithm, which samples rho nonzeros for each of the d columns (rho must be
        an integer >= 1). The location of nonzeros in each column is randomly chosen,
        and each nonzero location is assigned a value uniformly randomly over the interval
        [-1,1]. "frcn" is the same as "frc" except the nonzeros are sampled from
        the standard normal distribution. </p>
    </div>
    <h5> Value </h5>
    <p> random.matrix </p>
  </div>
    
    
  <span class="anchor" id = "RandMatCat"></span> 
  <div class = "docSect">
    <h4>RandMatCat - Create a Random Matrix for use when categorical features are present</h4>
    <h5> Description </h5>
    <p> The same as the function RandMat, except that this function is used when a cat.map object is
      provided (see RerF for details).</p>
    <h5> Usage </h5>
    <p><code> RandMatCat(mat.options) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> mat.options </code></p>
    </div>
    <div class = "argRight">
      <p> the same as that for RandMat, except an additional fifth argument cat.map is
        taken, which specifies which one-of-K encoded columns in X correspond to the
        same categorical feature. </p>
    </div>
    <h5> Value </h5>
    <p> random.matrix </p>
  </div>
    
  <span class="anchor" id = "Rerf"></span> 
  <div class = "docSect">
    <h4>RerF - RerF forest Generator</h4>
    <h5> Description </h5>
    <p> Creates a decision forest based on an input matrix and class vector. This is the main function in the
      rerf package. </p>
    <h5> Usage </h5>
    <p><code> RerF(X, Y, min.parent = 6L, trees = 500L,
      max.depth = ceiling(log2(nrow(X))), bagging = 0.2, replacement = TRUE,
      stratify = FALSE, fun = NULL, mat.options = list(p =
      ifelse(is.null(cat.map), ncol(X), length(cat.map)), d =
      ceiling(sqrt(ncol(X))), random.matrix = "binary", rho =
      ifelse(is.null(cat.map), 1/ncol(X), 1/length(cat.map)), prob = 0.5),
      rank.transform = FALSE, store.oob = FALSE, store.impurity = FALSE,
      progress = FALSE, rotate = F, num.cores = 0L, seed = sample(0:1e+08,
      1), cat.map = NULL, rfPack = FALSE)</code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> Y </code></p>
      <p><code> min.parent </code></p>
      <p><code> trees </code></p>
      <p><code> max.depth </code></p>
      <p><code> bagging </code></p>
      <p><code> replacement </code></p>
      <p><code> stratify </code></p>
      <p style = "height: 150px;"><code> fun </code></p>
      <p><code> mat.options </code></p>
      <p><code> rank.transform </code></p>
      <p><code> store.oob </code></p>
      <p><code> store.impurity </code></p>
      <p><code> progress </code></p>
      <p><code> rotate </code></p>
      <p><code> num.cores </code></p>
      <p><code> seed </code></p>
      <p style = "height: 250px;"><code> cat.map </code></p>
      <p><code> rfPack </code></p>
      <p><code> prob </code></p>

    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame. The rows correspond to
        observations and columns correspond to features. </p>
      <p> an n length vector of class labels. Class labels must be integer or numeric and
        be within the range 1 to the number of classes. </p>
      <p> the minimum splittable node size. A node size < min.parent will be a leaf node. (min.parent = 6) </p>
      <p> the number of trees in the forest. (trees=500) </p>
      <p> the longest allowable distance from the root of a tree to a leaf node (i.e. the maximum allowed height for a tree). If max.depth=0, the tree will be allowed to grow without bound. (max.depth=ceiling(log2(nrow(X))) ) </p>
      <p> a non-zero value means a random sample of X will be used during tree creation. If replacement = FALSE the bagging value determines the percentage of samples to leave out-of-bag. If replacement = TRUE the non-zero bagging value is ignored. (bagging=.2)</p>
      <p> if TRUE then n samples are chosen, with replacement, from X. (replacement=TRUE) </p>
      <p> if TRUE then class sample proportions are maintained during the random sampling. Ignored if replacement = FALSE. (stratify = FALSE). </p>
      <p style = "height: 150px;"> a function that creates the random projection matrix. If NULL and cat.map is NULL, then RandMat is used. If NULL and cat.map is not NULL, then
        RandMatCat is used, which adjusts the sampling of features when categorical features have been one-of-K encoded. If a custom function is to be used, then it
        must return a matrix in sparse representation, in which each nonzero is an array of the form (row.index, column.index, value). See RandMat or RandMatCat for
        details. (fun=NULL)</p>
      <p> a list of parameters to be used by fun. (mat.options=c(ncol(X), round(ncol(X)^.5),1L, 1/ncol(X))) </p>
      <p> if TRUE then each feature is rank-transformed (i.e. smallest value becomes 1 and largest value becomes n) (rank.transform=FALSE) </p>
      <p> if TRUE then the samples omitted during the creation of a tree are stored as part of the tree. This is required to run OOBPredict(). (store.oob=FALSE)</p>
      <p> if TRUE then the decrease in impurity is stored for each split. This is required to run FeatureImportance() (store.impurity=FALSE) </p>
      <p> if TRUE then a pipe is printed after each tree is created. This is useful for large datasets. (progress=FALSE) </p>
      <p> if TRUE then the data matrix X is uniformly randomly rotated for each tree. (rotate=FALSE) </p>
      <p> the number of cores to use while training. If num.cores=0 then 1 less than the number of cores reported by the OS are used. (num.cores=0) </p>
      <p> the seed to use for training the forest. For two runs to match you must use the same seed for each run AND you must also use the same number of cores for
        each run. (seed=sample((0:100000000,1))) </p>
      <p style = "height: 250px;"> a list specifying which columns in X correspond to the same one-of-K encoded feature. Each element of cat.map is a numeric vector specifying the K column
        indices of X corresponding to the same categorical feature after one-of-K encoding. All one-of-K encoded features in X must come after the numeric features.
        The K encoded columns corresponding to the same categorical feature must be placed contiguously within X. The reason for specifying cat.map is to adjust
        for the fact that one-of-K encoding cateogorical features results in a dilution of numeric features, since a single categorical feature is expanded to K binary features.
        If cat.map = NULL, then RerF assumes all features are numeric (i.e. none of the features have been one-of-K encoded). </p>
      <p> boolean flag to determine whether to pack a random forest in order to improve
        prediction speed. This flag is only applicable when training a forest with the "rf"
        option. (rfPack = FALSE) </p>
      <p> the probability of sampling +1 in the default random matrix function </p>
    </div>
    <h5> Value </h5>
      <p> forest </p>
    <h5> Examples </h5>
      <p><code> ### Train RerF on numeric data ### </code></p>
      <p><code> library(rerf) </code></p>
      <p><code> forest <- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L) </code></p>
      <p><code> ### Train RerF on one-of-K encoded categorical data ### </code></p>
      <p><code> df1 <- as.data.frame(Titanic) </code></p>
      <p><code> nc <- ncol(df1) </code></p>
      <p><code> df2 <- df1[NULL, -nc] </code></p>
      <p><code> for (i in which(df1$Freq != 0L)) { </code></p>
      <p><code> df2 <- rbind(df2, df1[rep(i, df1$Freq[i]), -nc]) }</code></p>
      <p><code> n <- nrow(df2) # number of observations </code></p>
      <p><code> p <- ncol(df2) - 1L # number of features </code></p>
      <p><code> num.categories <- apply(df2[, 1:p], 2, function(x) length(unique(x))) </code></p>
      <p><code> p.enc <- sum(num.categories) # number of features after one-of-K encoding </code></p>
      <p><code> X <- matrix(0, nrow = n, ncol = p.enc) # initialize training data matrix X </code></p>
      <p><code> cat.map <- vector("list", p) </code></p>
      <p><code> col.idx <- 0L </code></p>
      <p><code> # one-of-K encode each categorical feature and store in X </code></p>
      <p><code> for (j in 1:p) { </code></p>
      <p><code> cat.map[[j]] <- (col.idx + 1L):(col.idx + num.categories[j]) </code></p>
      <p><code> X[, cat.map[[j]]] <- dummies::dummy(df2[[j]]) # converts categorical feature to K dummy variables </code></p>
      <p><code> col.idx <- col.idx + num.categories[j] } </code></p>
      <p><code> Y <- df2$Survived </code></p>
      <p><code> # specifying the cat.map in RerF allows training to be aware of which dummy variables correspond </code></p>
      <p><code> # to the same categorical feature </code></p>
      <p><code> forest <- RerF(X, Y, num.cores = 1L, cat.map = cat.map) </code></p>
      <p><code> ### Train a random rotation ensemble of CART decision trees (see Blaser and Fryzlewicz 2016) ### </code></p>      
      <p><code> forest <- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L, mat.options = list(p=4, d=2,random.matrix="rf", 0.25), rotate = TRUE) </code></p>
  </div>  
    
  <span class="anchor" id = "RunFeatureImportance"></span> 
  <div class = "docSect">
    <h4>RunFeatureImportance - Compute Feature Importance of a single RerF tree</h4>
    <h5> Description </h5>
    <p> Computes feature importance of every unique feature used to make a split in a single tree.</p>
    <h5> Usage </h5>
    <p><code> RunFeatureImportance(tree, unique.projections) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> tree </code></p>
      <p><code> unique.projections </code></p>
    </div>
    <div class = "argRight">
      <p>a single tree from a trained RerF model with argument store.impurity = TRUE. </p>
      <p>a list of all of the unique split projections used in the RerF model. </p>
    </div>
    <h5> Value </h5>
    <p> feature.imp </p>
  </div>
    
  <span class="anchor" id = "RunOOB"></span> 
  <div class = "docSect">
    <h4>RunOOB - Predict class labels on out-of-bag observations using a single tree</h4>
    <h5> Description </h5>
    <p> This is the base function called by OOBPredict.</p>
    <h5> Usage </h5>
    <p><code> RunFeatureImportance(tree, unique.projections) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> tree </code></p>
    </div>
    <div class = "argRight">
      <p>an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest.</p>
      <p>a tree from a forest returned by RerF. </p>
    </div>
    <h5> Value </h5>
    <p> predictions prediction matrix used by OOBPredict </p>
  </div>
    
  <span class="anchor" id = "RunPredict"></span> 
  <div class = "docSect">
    <h4> RunPredict - Predict class labels on a test set using a single tree.</h4>
    <h5> Description </h5>
    <p> This is the base function called by Predict.</p>
    <h5> Usage </h5>
    <p><code> RunPredict(X, tree)</code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> tree </code></p>
    </div>
    <div class = "argRight">
      <p>an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest.</p>
      <p>a tree from a forest returned by RerF.</p>
    </div>
    <h5> Value </h5>
    <p> predictions an n length vector of prediction based on the tree provided to this function </p>
  </div>
   
  
  <span class="anchor" id = "RunPredictLeaf"></span> 
  <div class = "docSect">
    <h4> RunPredictLeaf - Calculate similarity using a single tree</h4>
    <h5> Description </h5>
    <p> This is the base function called by ComputeSimilarity.</p>
    <h5> Usage </h5>
    <p><code> RunPredictLeaf(X, tree) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> tree </code></p>
    </div>
    <div class = "argRight">
      <p>an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest.</p>
      <p>a tree from a forest returned by RerF.</p>
    </div>
    <h5> Value </h5>
    <p> similarity based on one tree </p>
  </div>
    
  
  <span class="anchor" id = "StrCorr"></span> 
  <div class = "docSect">
    <h4> StrCorr - Compute tree strength and correlation</h4>
    <h5> Description </h5>
    <p> Computes estimates of tree strength and correlation according to the definitions in Breiman’s 2001
      Random Forests paper.</p>
    <h5> Usage </h5>
    <p><code> StrCorr(Yhats, Y) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> Yhats </code></p>
      <p><code> Y </code></p>
    </div>
    <div class = "argRight">
      <p>predicted class labels for each tree in a forest</p>
      <p> true class labels.</p>
    </div>
    <h5> Examples </h5>
      <p><code> library(rerf) </code></p>
      <p><code> trainIdx <- c(1:40, 51:90, 101:140) </code></p>
      <p><code> X <- as.matrix(iris[,1:4]) </code></p>
      <p><code> Y <- iris[[5]] </code></p>
      <p><code> forest <- RerF(X[trainIdx, ], Y[trainIdx], num.cores = 1L) </code></p>
      <p><code> predictions <- Predict(X[-trainIdx, ], forest, num.cores = 1L, aggregate.output = FALSE) </code></p>
      <p><code> scor <- StrCorr(predictions, Y[-trainIdx]) </code></p>
  </div>     
<div>
</div>
<script>
function openNav() {
  document.getElementById("mySidenav").style.width = "250px";
  document.getElementById("myMenu").style.opacity = "0";
}

function closeNav() {
  document.getElementById("mySidenav").style.width = "0";
  document.getElementById("myMenu").style.opacity = "1";
}
</script>

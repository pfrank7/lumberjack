---
layout: page
title : Docs
header : Docs
order : 10
group : navigation
description: "Lumberjack Docs"
---
{% include JB/setup %}
<div>

<div class="sidenav">
  <a href = "#About">About</a>
  <a href="#BuildTree">BuildTree</a>
  <a href="#ComputeSimilarity">ComputeSimilarity</a>
  <a href="#FeatureImportance">FeatureImportance</a>
  <a href="#mnist">mnist</a>
  <a href="#OOBPredict">OOBPredict</a>
  <a href="#PackForest">PackForest</a>
  <a href="#PackPredict">PackPredict</a>
  <a href="#Predict">Predict</a>
  <a href="#RandMat">RandMat</a>
  <a href="#RandMatCat">RandMatCat</a>
  <a href="#RerF">RerF</a>
  <a href="#RunFeatureImportance">RunFeatureImportance</a>
  <a href="#RunOOB">RunOOB</a>
  <a href="#RunPredict">RunPredict</a>
  <a href="#RunPredictLeaf">RunPredictLeaf</a>
  <a href="#StrCorr">StrCorr</a>
</div>

<div class="main">
  <h2>Package 'rerf'</h2>
  <p>Navigate through the links on the left. The full pdf documentation can be found on <a href = "https://cran.r-project.org/web/packages/rerf/rerf.pdf"> cran. </a></p>
  <span class="anchor" id="About"></span>
  <div class = "aboutSect"> 
    <h3> About </h3> 
    <p><b> Type </b>Package</p>
    <p><b> Title </b>Randomer Forest </p>
    <p><b> Version </b>1.1.3</p>
    <p><b> Date </b>2018-07-05</p>
    <p><b> Description </b> Random Forester (RerF) is an algorithm developed by Tomita
      (2016) <arXiv:1506.03410v2> which is similar to Random Forest - Random
      Combination (Forest-RC) developed by Breiman (2001)
      <doi:10.1023/A:1010933404324>. Random Forests create axis-parallel, or
      orthogonal trees. That is, the feature space is recursively split along
      directions parallel to the axes of the feature space. Thus, in cases in
      which the classes seem inseparable along any single dimension, Random
      Forests may be suboptimal. To address this, Breiman also proposed and
      characterized Forest-RC, which uses linear combinations of coordinates
      rather than individual coordinates, to split along. This package,
      'rerf', implements RerF which is similar to Forest-RC. The difference
      between the two algorithms is where the random linear combinations
      occur: Forest-RC combines features at the per tree level whereas RerF
      takes linear combinations of coordinates at every node in the tree.</p>
      <p><b> License </b>GPL-2</p>
    <p><b> URL </b><a hrerf = "https://github.com/neurodata/R-RerF">https://github.com/neurodata/R-RerF</a></p>
    <p><b> Imports </b>parallel, RcppZiggurat, utils, stats, dummies</p>
    <p><b> LinkingTo </b>Rcpp, RcppArmadillo</p>
    <p><b> SystemRequirements </b>GNU make</p>
    <p><b> ByteCompile </b>true</p>
    <p><b> RoxygenNote </b>6.0.1</p>
    <p><b> NeedsCompilation </b>yes</p>
    <p><b> Author </b>James Browne [aut, cre],
     Tyler Tomita [aut],
     Joshua Vogelstein [ths]</p>
    <p><b> Maintainer </b>James Browne <i>jbrowne6@jhu.edu</i> </p>
    <p><b> Depends </b>R (>= 2.10)</p>
    <p><b> Repository </b>CRAN</p>
    <p><b> Date/Publication </b>2018-07-29 06:10:03 UTC</p>
  
  
  </div>
  <span class="anchor" id = "BuildTree"></span>
  <div class = "docSect">
    <h4> BuildTree - RerF Tree Generator </h4>
    <h5> Description </h5>
    <p> Creates a single decision tree based on an input matrix and class vector. This is the function used
      by rerf to generate trees.</p>
    <h5> Usage </h5>
    <p><code> BuildTree(X, Y, min.parent, max.depth, bagging, replacement, stratify,
      class.ind, class.ct, fun, mat.options, store.oob, store.impurity, progress,
        rotate) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> Y </code></p>
      <p><code> min.parent </code></p>
      <p><code> max.depth </code></p>
      <p><code> bagging </code></p>
      <p><code> replacement </code></p>
      <p><code> stratify </code></p>
      <p><code> class.ind </code></p>
      <p><code> class.ct  </code></p>
      <p><code> fun </code></p>
      <p><code> mat.options  </code></p>
      <p><code> store.oob </code></p>
      <p><code> store.impurity  </code></p>
      <p><code> progress  </code></p>
      <p><code> rotate </code></p>
    </div>
    <div class = "argRight">
      <p> an n by d numeric matrix (preferable) or data frame. The rows correspond to
        observations and columns correspond to features. </p>
      <p> an n length vector of class labels. Class labels must be integer or numeric and
        be within the range 1 to the number of classes. </p>
      <p> the minimum splittable node size. A node size < min.parent will be a leaf node.
        (min.parent = 6) </p>
      <p> the longest allowable distance from the root of a tree to a leaf node (i.e. the
        maximum allowed height for a tree). If max.depth=0, the tree will be allowed to
        grow without bound. (max.depth=0) </p>
      <p> a non-zero value means a random sample of X will be used during tree creation.
        If replacement = FALSE the bagging value determines the percentage of samples
        to leave out-of-bag. If replacement = TRUE the non-zero bagging value is
        ignored. (bagging=.2) </p>
      <p> if TRUE then n samples are chosen, with replacement, from X. (replacement=TRUE) </p>
      <p> if TRUE then class sample proportions are maintained during the random sampling.
        Ignored if replacement = FALSE. (stratify = FALSE). </p>
      <p> a vector of lists. Each list holds the indexes of its respective class (e.g. list 1
        contains the index of each class 1 sample). </p>
      <p> a cumulative sum of class counts. </p>
      <p> a function that creates the random projection matrix. (fun=NULL) </p>
      <p> a list of parameters to be used by fun. (mat.options=c(ncol(X), round(ncol(X)^.5),1L,
        1/ncol(X))) </p>
      <p> if TRUE then the samples omitted during the creation of a tree are stored as part
        of the tree. This is required to run OOBPredict(). (store.oob=FALSE) </p>
      <p> if TRUE then the reduction in Gini impurity is stored for every split. This is
        required to run FeatureImportance() (store.impurity=FALSE) </p>
      <p> if true a pipe is printed after each tree is created. This is useful for large datasets.
        (progress=FALSE) </p>
      <p> if TRUE then the data matrix X is uniformly randomly rotated. (rotate=FALSE) </p>
      
    </div>
    <h5> Value </h5>
    <p> Tree </p>
  </div>
  <span class="anchor" id = "ComputeSimilarity"></span>
  <div class = "docSect">
    <h4> ComputeSimilarity - Compute Similarities </h4>
    <h5> Description </h5>
    <p> Computes pairwise similarities between observations. The similarity between two points is defined
      as the fraction of trees such that two points fall into the same leaf node.</p>
    <h5> Usage </h5>
    <p><code> ComputeSimilarity(X, forest, num.cores = 0L, Xtrain = NULL) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> X </code></p>
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
      <p><code> Xtrain </code></p>
    </div>
    <div class = "argRight">
      <p> an n sample by d feature matrix (preferable) or data frame which was used to
        train the provided forest. </p>
      <p> forest a forest trained using the rerf function, with COOB=TRUE. </p>
      <p> num.cores the number of cores to use while training. If num.cores=0 then 1 less 
        than the number of cores reported by the OS are used. (num.cores=0) </p>
      <p> Xtrain an n by d numeric matrix (preferable) or data frame. This should be the same data matrix/frame used 
        to train the forest, and is only required if RerF was called with rank.transform = TRUE. (Xtrain=NULL) </p>
    </div>
    <h5> Value </h5>
    <p> similarity a normalized n by n matrix of pairwise similarities </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> X &lt- as.matrix(iris[,1:4]) </code></p>
    <p><code> Y &lt- iris[[5L]] </code></p>
    <p><code> forest &lt- RerF(X, Y, num.cores = 1L) </code></p>
    <p><code> sim.matrix &lt- ComputeSimilarity(X, forest, num.cores = 1L) </code></p> 
  </div>
  <span class="anchor" id = "FeatureImportance"></span>
  <div class = "docSect">
    <h4> FeatureImportance - Compute Feature Importance of a RerF model </h4>
    <h5> Description </h5>
    <p> Computes feature importance of every unique feature used to make a split in the RerF model.</p>
    <h5> Usage </h5>
    <p><code> FeatureImportance(forest, num.cores = 0L) </code></p>
    <h5> Arguments </h5>
    <div class = "argLeft">
      <p><code> forest </code></p>
      <p><code> num.cores </code></p>
    </div>
    <div class = "argRight">
      <p> a forest trained using the RerF function with argument store.impurity = TRUE </p>
      <p> number of cores to use. If num.cores = 0, then 1 less than the number of cores
        reported by the OS are used. (num.cores = 0) </p>
    </div>
    <h5> Value </h5>
    <p> feature.imp </p>
    <h5> Examples </h5>
    <p><code> library(rerf)  </code></p>
    <p><code> forest &lt- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L, store.impurity = TRUE) </code></p>
    <p><code> feature.imp &lt- FeatureImportance(forest, num.cores = 1L) </code></p>
  </div>
  <span class="anchor" id = "mnist"></span>    
  <!--div>mnist - A subset of the MNIST dataset for handwritten digit classification</div>
  <div id = "OOBPredict">OOBPredict - Compute out-of-bag predictions</div>
  <div id = "PackForest">PackForest - Packs a forest and saves modified forest to disk for use by PackPredict
function</div>
  <div id = "PackPredict">PackPredict - Compute class predictions for each observation in X</div>
  <div id = "Predict">Predict - Compute class predictions for each observation in X</div>
  <div id = "RandMat">RandMat - Create a Random Matrix</div>
  <div id = "RandMatCat">RandMatCat - Create a Random Matrix for use when categorical features are present</div>
  <div id = "RerF">RerF - RerF forest Generator</div>
  <div id = "RunFeatureImportance">RunFeatureImportance - Compute Feature Importance of a single RerF tree</div>
  <div id = "RunOOB">RunOOB - Predict class labels on out-of-bag observations using a single tree</div>
  <div id = "RunPredict">RunPredict - Predict class labels on a test set using a single tree.</div>
  <div id = "RunPredictLeaf">RunPredictLeaf - Calculate similarity using a single tree</div>
  <div id = "StrCorr">StrCorr - Compute tree strength and correlation</div-->
</div>
     
</div>
